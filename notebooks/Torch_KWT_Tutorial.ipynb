{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2TvhbFZpR0dz"
      },
      "source": [
        "# Torch-KWT Tutorial\n",
        "\n",
        "This notebook will guide you through the steps to training and running inference on Google Speech Commands V2 (35) with the [Torch-KWT](https://github.com/ID56/Torch-KWT) repository."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktCvR-FJT9Zl"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UTOUEB7VR_co"
      },
      "source": [
        "### 1. Clone the repository"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FpEyVD_pRsLK",
        "outputId": "4f103f42-bf7c-4782-c051-919939b42766"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'Torch-KWT'...\n",
            "remote: Enumerating objects: 99, done.\u001b[K\n",
            "remote: Counting objects: 100% (99/99), done.\u001b[K\n",
            "remote: Compressing objects: 100% (75/75), done.\u001b[K\n",
            "remote: Total 99 (delta 41), reused 67 (delta 19), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (99/99), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/ID56/Torch-KWT.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oMyHNXrDSId5",
        "outputId": "64c73c06-a60f-42bb-d72e-a9a99a81e3d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/Torch-KWT\n"
          ]
        }
      ],
      "source": [
        "cd Torch-KWT/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8V1TojnpSNKO"
      },
      "source": [
        "### 2. Install requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pYxEu6sUSJGw",
        "outputId": "c1b8f40d-a6d5-404b-a7a8-36d8d16cf459"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 636 kB 5.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.7 MB 40.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 133 kB 52.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 170 kB 45.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 97 kB 6.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.7 MB/s \n",
            "\u001b[?25h  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -qr requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3vqXzDSSdsW"
      },
      "source": [
        "### 3. Download the Google Speech Commands V2 dataset\n",
        "\n",
        "We'll be saving it to the `./data/` folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kyTh1owVSeoa",
        "outputId": "01d0a18d-6670-47ec-81ce-81f340ab8b1e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2021-08-28 16:09:35--  http://download.tensorflow.org/data/speech_commands_v0.02.tar.gz\n",
            "Resolving download.tensorflow.org (download.tensorflow.org)... 142.250.152.128, 2607:f8b0:4001:c56::80\n",
            "Connecting to download.tensorflow.org (download.tensorflow.org)|142.250.152.128|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2428923189 (2.3G) [application/gzip]\n",
            "Saving to: ‘STDOUT’\n",
            "\n",
            "-                   100%[===================>]   2.26G  33.0MB/s    in 57s     \n",
            "\n",
            "2021-08-28 16:10:32 (40.8 MB/s) - written to stdout [2428923189/2428923189]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!sh ./download_gspeech_v2.sh ./data/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xqgYjtHXSslk",
        "outputId": "de621ca0-dab7-45f4-fed1-62fcbb701eb0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "_background_noise_  five     left     README.md\t\ttree\n",
            "backward\t    follow   LICENSE  right\t\ttwo\n",
            "bed\t\t    forward  marvin   seven\t\tup\n",
            "bird\t\t    four     nine     sheila\t\tvalidation_list.txt\n",
            "cat\t\t    go\t     no       six\t\tvisual\n",
            "dog\t\t    happy    off      stop\t\twow\n",
            "down\t\t    house    on       testing_list.txt\tyes\n",
            "eight\t\t    learn    one      three\t\tzero\n"
          ]
        }
      ],
      "source": [
        "!ls data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "leL0YA9STIua"
      },
      "source": [
        "As you can see, the dataset provides a `validation_list.txt` and a `testing_list.txt` as the split. We'll run a simple script `make_data_list.py` to also generate a `training_list.txt`, as well as a `label_map.json` that maps numeric indices to class labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TdRfkTruTFCn",
        "outputId": "44698d85-3ae2-4a91-dfbd-347ed2cd5be2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of training samples: 84843\n",
            "Number of validation samples: 9981\n",
            "Number of test samples: 11005\n",
            "Saved data lists and label map.\n"
          ]
        }
      ],
      "source": [
        "!python make_data_list.py -v ./data/validation_list.txt -t ./data/testing_list.txt -d ./data/ -o ./data/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUYdSxgpY6Mm"
      },
      "source": [
        "## Training\n",
        "\n",
        "For training, we only need to provide the config file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oS8NAS4kclsA"
      },
      "source": [
        "### 9. Setting Up Your Config File\n",
        "\n",
        "For this example, we'll be using the `sample_configs/base_config.yaml`. In fact, you should be able to use this config to reproduce the results of the provided pretrained KWT-1 checkpoint if you follow the exact settings (training for 140 epochs / ~23000 steps @ batch_size = 512).\n",
        "\n",
        "We'll be training for 10 epochs in this example.\n",
        "\n",
        "You can also use [wandb](wandb.ai) to log your runs. Either provide a path to a txt file containing your API key, or set the env variable \"WANDB_API_KEY\", like:\n",
        "\n",
        "```\n",
        "os.environ[\"WANDB_API_KEY\"] = \"yourkey\"\n",
        "```\n",
        "\n",
        "We will not be using wandb in this example, but feel free to try it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q3fYqLQQYlds"
      },
      "outputs": [],
      "source": [
        "conf_str = \"\"\"# sample config to run a demo training of 20 epochs\n",
        "\n",
        "data_root: ./data/\n",
        "train_list_file: ./data/training_list.txt\n",
        "val_list_file: ./data/validation_list.txt\n",
        "test_list_file: ./data/testing_list.txt\n",
        "label_map: ./data/label_map.json\n",
        "\n",
        "exp:\n",
        "    wandb: False\n",
        "    wandb_api_key: <path/to/api/key>\n",
        "    proj_name: torch-kwt-1\n",
        "    exp_dir: ./runs\n",
        "    exp_name: exp-0.0.1\n",
        "    device: auto\n",
        "    log_freq: 20    # log every l_f steps\n",
        "    log_to_file: True\n",
        "    log_to_stdout: True\n",
        "    val_freq: 1    # validate every v_f epochs\n",
        "    n_workers: 1\n",
        "    pin_memory: True\n",
        "    cache: 2 # 0 -> no cache | 1 -> cache wavs | 2 -> cache specs; stops wav augments\n",
        "    model_mode: 0 # 0 -> mfcc filters | 1 -> custom adaptive filters | 2 -> custom mfcc filters\n",
        "    \n",
        "\n",
        "hparams:\n",
        "    seed: 0\n",
        "    batch_size: 512\n",
        "    n_epochs: 10\n",
        "    l_smooth: 0.1\n",
        "\n",
        "    audio:\n",
        "        sr: 16000\n",
        "        n_mels: 40\n",
        "        n_fft: 480\n",
        "        win_length: 480\n",
        "        hop_length: 160\n",
        "        center: False\n",
        "    \n",
        "    model:\n",
        "        name: # if name is provided below settings will be ignored during model creation   \n",
        "        input_res: [40, 98]\n",
        "        patch_res: [40, 1]\n",
        "        num_classes: 35\n",
        "        mlp_dim: 256\n",
        "        dim: 64\n",
        "        heads: 1\n",
        "        depth: 12\n",
        "        dropout: 0.0\n",
        "        emb_dropout: 0.1\n",
        "        pre_norm: False\n",
        "\n",
        "    optimizer:\n",
        "        opt_type: adamw\n",
        "        opt_kwargs:\n",
        "            lr: 0.001\n",
        "            weight_decay: 0.1\n",
        "    \n",
        "    scheduler:\n",
        "        n_warmup: 10\n",
        "        max_epochs: 140\n",
        "        scheduler_type: cosine_annealing\n",
        "\n",
        "    augment:\n",
        "        # resample:\n",
        "            # r_min: 0.85\n",
        "            # r_max: 1.15\n",
        "        \n",
        "        # time_shift:\n",
        "            # s_min: -0.1\n",
        "            # s_max: 0.1\n",
        "\n",
        "        # bg_noise:\n",
        "            # bg_folder: ./data/_background_noise_/\n",
        "\n",
        "        spec_aug:\n",
        "            n_time_masks: 2\n",
        "            time_mask_width: 25\n",
        "            n_freq_masks: 2\n",
        "            freq_mask_width: 7\"\"\"\n",
        "\n",
        "!mkdir -p configs\n",
        "with open(\"configs/kwt1_colab.yaml\", \"w+\") as f:\n",
        "    f.write(conf_str)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FX6yUTn2cyaF"
      },
      "source": [
        "### 10. Initiating Training\n",
        "\n",
        "Make sure you are using a GPU runtime.\n",
        "\n",
        "In order to train to a full 140 epochs / 23000 steps like the paper, on free resources, we need to cut down on disk I/O and audio processing time. So, we'll preemptively convert all our `.wav` files into MFCCs of shape `(40, 98)` and keep them stored in memory. This caching process may take ~6 minutes.\n",
        "\n",
        "Since we'll be directly using MFCCs, no wav augmentations like resample, time_shift or background_noise will be used; we'll just use spectral augmentation with the settings from the paper."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSasLdmVnmxZ"
      },
      "source": [
        "\n",
        "\n",
        "> Note: You may notice a \"Warning: Leaking Caffe2 thread-pool after fork.\" message after each epoch. It seems to be an existing torch-1.9 issue, which you can ignore. [See more here.](https://github.com/pytorch/pytorch/issues/57273)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mZhtrxH4cQx2",
        "outputId": "10ac15c1-4e0a-43a0-9d9f-05fd1e860649"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Set seed 0\n",
            "Using settings:\n",
            " data_root: ./data/\n",
            "exp:\n",
            "  cache: 2\n",
            "  device: &id001 !!python/object/apply:torch.device\n",
            "  - cuda\n",
            "  exp_dir: ./runs\n",
            "  exp_name: exp-0.0.1\n",
            "  log_freq: 20\n",
            "  log_to_file: true\n",
            "  log_to_stdout: true\n",
            "  n_workers: 1\n",
            "  pin_memory: true\n",
            "  proj_name: torch-kwt-1\n",
            "  save_dir: ./runs/exp-0.0.1\n",
            "  val_freq: 1\n",
            "  wandb: false\n",
            "  wandb_api_key: <path/to/api/key>\n",
            "hparams:\n",
            "  audio:\n",
            "    center: false\n",
            "    hop_length: 160\n",
            "    n_fft: 480\n",
            "    n_mels: 40\n",
            "    sr: 16000\n",
            "    win_length: 480\n",
            "  augment:\n",
            "    spec_aug:\n",
            "      freq_mask_width: 7\n",
            "      n_freq_masks: 2\n",
            "      n_time_masks: 2\n",
            "      time_mask_width: 25\n",
            "  batch_size: 512\n",
            "  device: *id001\n",
            "  l_smooth: 0.1\n",
            "  model:\n",
            "    depth: 12\n",
            "    dim: 64\n",
            "    dropout: 0.0\n",
            "    emb_dropout: 0.1\n",
            "    heads: 1\n",
            "    input_res:\n",
            "    - 40\n",
            "    - 98\n",
            "    mlp_dim: 256\n",
            "    name: null\n",
            "    num_classes: 35\n",
            "    patch_res:\n",
            "    - 40\n",
            "    - 1\n",
            "    pre_norm: false\n",
            "  n_epochs: 10\n",
            "  optimizer:\n",
            "    opt_kwargs:\n",
            "      lr: 0.001\n",
            "      weight_decay: 0.1\n",
            "    opt_type: adamw\n",
            "  scheduler:\n",
            "    max_epochs: 140\n",
            "    n_warmup: 10\n",
            "    scheduler_type: cosine_annealing\n",
            "  seed: 0\n",
            "label_map: ./data/label_map.json\n",
            "test_list_file: ./data/testing_list.txt\n",
            "train_list_file: ./data/training_list.txt\n",
            "val_list_file: ./data/validation_list.txt\n",
            "\n",
            "Caching dataset into memory.\n",
            "100% 84843/84843 [05:56<00:00, 238.19it/s]\n",
            "Caching dataset into memory.\n",
            "100% 9981/9981 [00:42<00:00, 236.75it/s]\n",
            "Created model with 559011 parameters.\n",
            "Initiating training.\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "Step: 0 | epoch: 0 | loss: 3.7112817764282227 | lr: 6.024096385505879e-07\n",
            "Step: 20 | epoch: 0 | loss: 3.663778781890869 | lr: 1.2650602409562347e-05\n",
            "Step: 40 | epoch: 0 | loss: 3.6317496299743652 | lr: 2.4698795180574107e-05\n",
            "Step: 60 | epoch: 0 | loss: 3.5630621910095215 | lr: 3.6746987951585866e-05\n",
            "Step: 80 | epoch: 0 | loss: 3.5519092082977295 | lr: 4.879518072259762e-05\n",
            "Step: 100 | epoch: 0 | loss: 3.516566753387451 | lr: 6.0843373493609386e-05\n",
            "Step: 120 | epoch: 0 | loss: 3.50459885597229 | lr: 7.289156626462113e-05\n",
            "Step: 140 | epoch: 0 | loss: 3.487091064453125 | lr: 8.493975903563291e-05\n",
            "Step: 160 | epoch: 0 | loss: 3.5054194927215576 | lr: 9.698795180664466e-05\n",
            "Step: 166 | epoch: 0 | time_per_epoch: 84.79703974723816 | train_acc: 0.04304421107221574 | avg_loss_per_ep: 3.56347295893244\n",
            "  0% 0/20 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "100% 20/20 [00:03<00:00,  5.52it/s]\n",
            "Step: 166 | epoch: 0 | val_loss: 3.4742739677429197 | val_acc: 0.05520488928965034\n",
            "Saved ./runs/exp-0.0.1/best.pth with accuracy 0.05520488928965034.\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "Step: 180 | epoch: 1 | loss: 3.4744887351989746 | lr: 0.00010903614457765641\n",
            "Step: 200 | epoch: 1 | loss: 3.457651138305664 | lr: 0.00012108433734866819\n",
            "Step: 220 | epoch: 1 | loss: 3.405949354171753 | lr: 0.00013313253011967992\n",
            "Step: 240 | epoch: 1 | loss: 3.3611838817596436 | lr: 0.00014518072289069169\n",
            "Step: 260 | epoch: 1 | loss: 3.3326175212860107 | lr: 0.00015722891566170345\n",
            "Step: 280 | epoch: 1 | loss: 3.331192970275879 | lr: 0.00016927710843271524\n",
            "Step: 300 | epoch: 1 | loss: 3.251941680908203 | lr: 0.00018132530120372697\n",
            "Step: 320 | epoch: 1 | loss: 3.2051000595092773 | lr: 0.00019337349397473874\n",
            "Step: 332 | epoch: 1 | time_per_epoch: 84.55368280410767 | train_acc: 0.10184694082010301 | avg_loss_per_ep: 3.3325931379594\n",
            "  0% 0/20 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "100% 20/20 [00:03<00:00,  5.53it/s]\n",
            "Step: 332 | epoch: 1 | val_loss: 2.953847825527191 | val_acc: 0.20809538122432622\n",
            "Saved ./runs/exp-0.0.1/best.pth with accuracy 0.20809538122432622.\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "Step: 340 | epoch: 2 | loss: 3.1043946743011475 | lr: 0.0002054216867457505\n",
            "Step: 360 | epoch: 2 | loss: 3.0105576515197754 | lr: 0.00021746987951676224\n",
            "Step: 380 | epoch: 2 | loss: 2.9756202697753906 | lr: 0.000229518072287774\n",
            "Step: 400 | epoch: 2 | loss: 2.8972558975219727 | lr: 0.00024156626505878576\n",
            "Step: 420 | epoch: 2 | loss: 2.9025986194610596 | lr: 0.0002536144578297975\n",
            "Step: 440 | epoch: 2 | loss: 2.8347620964050293 | lr: 0.0002656626506008093\n",
            "Step: 460 | epoch: 2 | loss: 2.8441624641418457 | lr: 0.000277710843371821\n",
            "Step: 480 | epoch: 2 | loss: 2.77036714553833 | lr: 0.00028975903614283276\n",
            "Step: 498 | epoch: 2 | time_per_epoch: 84.4742956161499 | train_acc: 0.22402555308039557 | avg_loss_per_ep: 2.9174649528710237\n",
            "  0% 0/20 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "100% 20/20 [00:03<00:00,  5.53it/s]\n",
            "Step: 498 | epoch: 2 | val_loss: 2.3710322260856627 | val_acc: 0.4090772467688608\n",
            "Saved ./runs/exp-0.0.1/best.pth with accuracy 0.4090772467688608.\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "Step: 500 | epoch: 3 | loss: 2.7047626972198486 | lr: 0.00030180722891384455\n",
            "Step: 520 | epoch: 3 | loss: 2.6720032691955566 | lr: 0.00031385542168485634\n",
            "Step: 540 | epoch: 3 | loss: 2.6616950035095215 | lr: 0.0003259036144558681\n",
            "Step: 560 | epoch: 3 | loss: 2.572598457336426 | lr: 0.00033795180722687987\n",
            "Step: 580 | epoch: 3 | loss: 2.58603572845459 | lr: 0.00034999999999789155\n",
            "Step: 600 | epoch: 3 | loss: 2.4438838958740234 | lr: 0.00036204819276890334\n",
            "Step: 620 | epoch: 3 | loss: 2.460984706878662 | lr: 0.00037409638553991513\n",
            "Step: 640 | epoch: 3 | loss: 2.449341058731079 | lr: 0.00038614457831092686\n",
            "Step: 660 | epoch: 3 | loss: 2.395538330078125 | lr: 0.00039819277108193865\n",
            "Step: 664 | epoch: 3 | time_per_epoch: 84.56505489349365 | train_acc: 0.33917942552715014 | avg_loss_per_ep: 2.5784010915871125\n",
            "  0% 0/20 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "100% 20/20 [00:03<00:00,  5.52it/s]\n",
            "Step: 664 | epoch: 3 | val_loss: 1.9651696264743805 | val_acc: 0.5543532712153091\n",
            "Saved ./runs/exp-0.0.1/best.pth with accuracy 0.5543532712153091.\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "Step: 680 | epoch: 4 | loss: 2.4336676597595215 | lr: 0.0004102409638529504\n",
            "Step: 700 | epoch: 4 | loss: 2.43867826461792 | lr: 0.0004222891566239622\n",
            "Step: 720 | epoch: 4 | loss: 2.3999013900756836 | lr: 0.00043433734939497386\n",
            "Step: 740 | epoch: 4 | loss: 2.403538703918457 | lr: 0.00044638554216598565\n",
            "Step: 760 | epoch: 4 | loss: 2.3347842693328857 | lr: 0.00045843373493699744\n",
            "Step: 780 | epoch: 4 | loss: 2.303086519241333 | lr: 0.0004704819277080092\n",
            "Step: 800 | epoch: 4 | loss: 2.308474540710449 | lr: 0.00048253012047902097\n",
            "Step: 820 | epoch: 4 | loss: 2.2181482315063477 | lr: 0.0004945783132500328\n",
            "Step: 830 | epoch: 4 | time_per_epoch: 84.54047894477844 | train_acc: 0.41416498709380856 | avg_loss_per_ep: 2.3563639184078538\n",
            "  0% 0/20 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "100% 20/20 [00:03<00:00,  5.53it/s]\n",
            "Step: 830 | epoch: 4 | val_loss: 1.7181882500648498 | val_acc: 0.6454263099889791\n",
            "Saved ./runs/exp-0.0.1/best.pth with accuracy 0.6454263099889791.\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "Step: 840 | epoch: 5 | loss: 2.2964859008789062 | lr: 0.0005066265060210444\n",
            "Step: 860 | epoch: 5 | loss: 2.256483554840088 | lr: 0.0005186746987920562\n",
            "Step: 880 | epoch: 5 | loss: 2.1767232418060303 | lr: 0.000530722891563068\n",
            "Step: 900 | epoch: 5 | loss: 2.1989598274230957 | lr: 0.0005427710843340797\n",
            "Step: 920 | epoch: 5 | loss: 2.1924338340759277 | lr: 0.0005548192771050915\n",
            "Step: 940 | epoch: 5 | loss: 2.1345086097717285 | lr: 0.0005668674698761033\n",
            "Step: 960 | epoch: 5 | loss: 2.240764617919922 | lr: 0.000578915662647115\n",
            "Step: 980 | epoch: 5 | loss: 2.1754984855651855 | lr: 0.0005909638554181268\n",
            "Step: 996 | epoch: 5 | time_per_epoch: 84.54821038246155 | train_acc: 0.46556580978984713 | avg_loss_per_ep: 2.2059779540601983\n",
            "  0% 0/20 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "100% 20/20 [00:03<00:00,  5.51it/s]\n",
            "Step: 996 | epoch: 5 | val_loss: 1.6209136426448822 | val_acc: 0.6781885582606954\n",
            "Saved ./runs/exp-0.0.1/best.pth with accuracy 0.6781885582606954.\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "Step: 1000 | epoch: 6 | loss: 2.1613078117370605 | lr: 0.0006030120481891385\n",
            "Step: 1020 | epoch: 6 | loss: 2.1277143955230713 | lr: 0.0006150602409601503\n",
            "Step: 1040 | epoch: 6 | loss: 2.1773295402526855 | lr: 0.000627108433731162\n",
            "Step: 1060 | epoch: 6 | loss: 2.114619493484497 | lr: 0.0006391566265021738\n",
            "Step: 1080 | epoch: 6 | loss: 2.050036907196045 | lr: 0.0006512048192731855\n",
            "Step: 1100 | epoch: 6 | loss: 2.0366430282592773 | lr: 0.0006632530120441973\n",
            "Step: 1120 | epoch: 6 | loss: 1.967737078666687 | lr: 0.0006753012048152091\n",
            "Step: 1140 | epoch: 6 | loss: 2.0537619590759277 | lr: 0.0006873493975862209\n",
            "Step: 1160 | epoch: 6 | loss: 2.120933771133423 | lr: 0.0006993975903572326\n",
            "Step: 1162 | epoch: 6 | time_per_epoch: 84.60452628135681 | train_acc: 0.4982614947609113 | avg_loss_per_ep: 2.107130696974605\n",
            "  0% 0/20 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "100% 20/20 [00:03<00:00,  5.52it/s]\n",
            "Step: 1162 | epoch: 6 | val_loss: 1.507552045583725 | val_acc: 0.718465083658952\n",
            "Saved ./runs/exp-0.0.1/best.pth with accuracy 0.718465083658952.\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "Step: 1180 | epoch: 7 | loss: 2.104174852371216 | lr: 0.0007114457831282443\n",
            "Step: 1200 | epoch: 7 | loss: 2.1451516151428223 | lr: 0.0007234939758992561\n",
            "Step: 1220 | epoch: 7 | loss: 2.073594331741333 | lr: 0.0007355421686702679\n",
            "Step: 1240 | epoch: 7 | loss: 2.0264627933502197 | lr: 0.0007475903614412797\n",
            "Step: 1260 | epoch: 7 | loss: 2.0592803955078125 | lr: 0.0007596385542122915\n",
            "Step: 1280 | epoch: 7 | loss: 2.0175940990448 | lr: 0.0007716867469833031\n",
            "Step: 1300 | epoch: 7 | loss: 2.0993452072143555 | lr: 0.0007837349397543149\n",
            "Step: 1320 | epoch: 7 | loss: 2.0283970832824707 | lr: 0.0007957831325253266\n",
            "Step: 1328 | epoch: 7 | time_per_epoch: 84.5787262916565 | train_acc: 0.5277041123015452 | avg_loss_per_ep: 2.0251631262790726\n",
            "  0% 0/20 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "100% 20/20 [00:03<00:00,  5.51it/s]\n",
            "Step: 1328 | epoch: 7 | val_loss: 1.4189611375331879 | val_acc: 0.7463180042079952\n",
            "Saved ./runs/exp-0.0.1/best.pth with accuracy 0.7463180042079952.\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "Step: 1340 | epoch: 8 | loss: 2.1316304206848145 | lr: 0.0008078313252963384\n",
            "Step: 1360 | epoch: 8 | loss: 1.9870837926864624 | lr: 0.0008198795180673501\n",
            "Step: 1380 | epoch: 8 | loss: 2.024782657623291 | lr: 0.0008319277108383619\n",
            "Step: 1400 | epoch: 8 | loss: 1.968630075454712 | lr: 0.0008439759036093737\n",
            "Step: 1420 | epoch: 8 | loss: 1.8796778917312622 | lr: 0.0008560240963803855\n",
            "Step: 1440 | epoch: 8 | loss: 1.8930541276931763 | lr: 0.0008680722891513973\n",
            "Step: 1460 | epoch: 8 | loss: 1.9484331607818604 | lr: 0.0008801204819224091\n",
            "Step: 1480 | epoch: 8 | loss: 1.858421802520752 | lr: 0.0008921686746934207\n",
            "Step: 1494 | epoch: 8 | time_per_epoch: 84.5880377292633 | train_acc: 0.550004125266669 | avg_loss_per_ep: 1.9611211031316274\n",
            "  0% 0/20 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "100% 20/20 [00:03<00:00,  5.50it/s]\n",
            "Step: 1494 | epoch: 8 | val_loss: 1.3739748418331146 | val_acc: 0.763250175333133\n",
            "Saved ./runs/exp-0.0.1/best.pth with accuracy 0.763250175333133.\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "Step: 1500 | epoch: 9 | loss: 1.9706593751907349 | lr: 0.0009042168674644325\n",
            "Step: 1520 | epoch: 9 | loss: 1.875306248664856 | lr: 0.0009162650602354443\n",
            "Step: 1540 | epoch: 9 | loss: 1.9171719551086426 | lr: 0.000928313253006456\n",
            "Step: 1560 | epoch: 9 | loss: 1.873132348060608 | lr: 0.0009403614457774677\n",
            "Step: 1580 | epoch: 9 | loss: 1.9139788150787354 | lr: 0.0009524096385484795\n",
            "Step: 1600 | epoch: 9 | loss: 1.904958963394165 | lr: 0.0009644578313194913\n",
            "Step: 1620 | epoch: 9 | loss: 1.8959975242614746 | lr: 0.0009765060240905031\n",
            "Step: 1640 | epoch: 9 | loss: 1.9573323726654053 | lr: 0.0009885542168615149\n",
            "Step: 1660 | epoch: 9 | time_per_epoch: 84.58947491645813 | train_acc: 0.5721273410888347 | avg_loss_per_ep: 1.8990052342414856\n",
            "  0% 0/20 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "100% 20/20 [00:03<00:00,  5.52it/s]\n",
            "Step: 1660 | epoch: 9 | val_loss: 1.3126880049705505 | val_acc: 0.7899008115419297\n",
            "Saved ./runs/exp-0.0.1/best.pth with accuracy 0.7899008115419297.\n",
            "  0% 0/20 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "100% 20/20 [00:03<00:00,  5.50it/s]\n",
            "Step: 1660 | epoch: 9 | val_loss: 1.3126880049705505 | val_acc: 0.7899008115419297\n",
            "Saved ./runs/exp-0.0.1/last.pth with accuracy 0.7899008115419297.\n",
            "Caching dataset into memory.\n",
            "100% 11005/11005 [00:48<00:00, 225.80it/s]\n",
            "  0% 0/22 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "100% 22/22 [00:04<00:00,  5.34it/s]\n",
            "Step: 1826 | test_loss_last: 1.3642499338496814 | test_acc_last: 0.7652885052248978\n",
            "Best ckpt loaded.\n",
            "  0% 0/22 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "100% 22/22 [00:04<00:00,  5.48it/s]\n",
            "Step: 1826 | test_loss_best: 1.3642499338496814 | test_acc_best: 0.7652885052248978\n"
          ]
        }
      ],
      "source": [
        "# !python train.py --conf configs/kwt1_colab.yaml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2o9uS25poupC"
      },
      "source": [
        "After training 10 epochs, we have a validation accuracy of **~78.99%** and a test accuracy of **~76.52%**.\n",
        "\n",
        "In colab, it takes ~84s per epoch, with an additional ~3s for validation. To do a complete training like the paper (140 epochs / 23K steps) on colab, you'd thus need around **3.4 hours**.\n",
        "\n",
        "You may also try running Torch-KWT training on kaggle, which I've found to be notably faster. Full training takes less than **2 hours** there."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from train import caching_pipeline, import_model, import_optimization_methods, training_pipeline\n",
        "from config_parser import get_config\n",
        "from utils.misc import seed_everything\n",
        "\n",
        "config = get_config('configs/kwt1_colab.yaml')\n",
        "seed_everything(config[\"hparams\"][\"seed\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainloader, valloader = caching_pipeline(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = import_model(config)\n",
        "optimizer, criterion, schedulers = import_optimization_methods(config, model, trainloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "training_pipeline(config, model, optimizer, criterion, trainloader, valloader, schedulers)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Torch-KWT-Tutorial-V2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
